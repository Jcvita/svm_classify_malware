import numpy as np
import os
import json
from collections import OrderedDict

DATA_DIR = 'dataset'
FEATURE_OCCURANCE_THRESHOLD = 10

# class SVM:
#     @classmethod
#     def create_hyperplane(w, b):
#         hyperplane = []
#         return hyperplane
    
#     def __init__(self, hyperplane):
#         self.hyperplane = hyperplane
    
#     def fit(self, X, y):
#         pass
    
def load_samples(labels, samples_dir):
    """load labled samples and identify unique features

    Args:
        labels (str): file containing labels
        samples_dir (str): directory containing samples

    Returns:
        tuple: ( [([(type of feature, feature)], <mal or benign>, <mal family || None>)], { feat: type }, { family: occurrences } )
    """
    os.chdir(DATA_DIR)
    #todo can be optimized by sequential read and parse
    malware_samples = {x[0]: x[1] for x in list(map(lambda y: y.split(","), open(f'{labels}', 'r').readlines()[1:]))} # {sha256 filename: family)
    data = [] # [([(type of feature, feature)], <mal or benign>, <mal family || None>)]
    feats = OrderedDict() # { <feat>: {type: str, occurrences: int} }
    families = OrderedDict() # { <family>: occurrences }
    count = 0
    for hash, family in malware_samples.items():
        #insert into family dictionary
        if families.get(family) is None:
            families[family] = 0
        families[family] += 1
        
    #open sample and insert into data
    all_samples = os.listdir(samples_dir)
    num_samples = len(all_samples)
    os.chdir(samples_dir)
    for hash in all_samples:
        with open(hash, 'r') as sample:
            lines = sample.readlines() # features of the sample
            # data.append(list(map(lambda x: tuple(x.split("::")), sample)))
            sample.close()

            # Build a list of all unique features present in the dataset.
            features = []
            for line in lines:
                feat = line.split("::")
                # for some reason a ["/n"] feature appears somewhere
                if len(feat) == 1:
                    continue
                # save sample features to data
                features.append(tuple(feat))
                # add feature to dictionary
                if feats.get(feat[1]) is None:
                    feats[feat[1]] = {"type": feat[0], "occurrences": 1}
                else:
                    feats[feat[1]]["occurrences"] += 1
            data.append((features, 1 if malware_samples.get(hash) else 0, malware_samples.get(hash)))
        if count % (num_samples // 100) == 0:
            print(f'Samples {(count / num_samples) * 100}% loaded ')
        count += 1 
    
    return data, feats#, families

def vectorize(sample: tuple, relavent_feats):
    # relavent_feats: ordered list of features above threshold {key: vec_pos}
    # print(len(relavent)) # 544060 with no threshold, 177439 with 2 threshold, 17561 with 10 threshold
    feature_vector = [0]*len(relavent_feats)
    for feat in sample[0]:
        if relavent_feats.get(feat[1]):
            feature_vector[relavent_feats[feat[1]]] = 1 
    return feature_vector

def main():
    # 1. Dataset processing
    # data: [[((type of feature, feature)], <mal or benign>, <mal family || None>)]
    # feats: { <feat>: {type: str, occurrences: int} }
    print("loading samples...")
    data = OrderedDict()
    feats = OrderedDict()
    if "cache.json" in os.listdir() and "feats.json" in os.listdir():
        print("Loading data and feats from cache...")
        with open("cache.json", 'r') as cache:
            with open("feats.json", 'r') as feats_f:
                data = json.load(cache)
                feats = json.load(feats_f)
        feats_f.close()
        cache.close()
    else:
        data, feats = load_samples('sha256_family.csv', 'feature_vectors')
        os.chdir("../../")
        print("Caching data...")
        with open("cache.json", "w") as cache:
            json.dump(data, cache)
            cache.close()
        with open("feats.json", "w") as feats_f:
            json.dump(feats, feats_f)
            feats_f.close()
        print("Cache Complete!")
    print("Loading complete!")
    
    # For each sample, build a vector of 1s and 0s in which a 1 signals that the feature
    # was present in the sample and a 0 signals that it was not.
    # data: [([feature vector], <mal or benign>, <mal family || None>)]
    print("vectorizing data...")

    # relavent = [x for x in feats.keys() if feats[x]["occurrences"] >= FEATURE_OCCURANCE_THRESHOLD]
    relavent = OrderedDict()
    relavent_count = 0
    for k in feats.keys():
        if feats[k]["occurrences"] >= FEATURE_OCCURANCE_THRESHOLD:
            relavent[k] = relavent_count # don't think I need any more data for the relavent feats
            relavent_count += 1

    num_samples = len(data)
    for x in range(num_samples // 2):
        sample = list(data[x])
        #forced SIGKILL at 74% of samples at FOC = 10
        sample[0] = vectorize(sample, relavent)
        data[x] = tuple(sample)
        if x % (num_samples // 100) == 0:
            print(f'Samples {(x / num_samples) * 100}% vectorized ')

    print(data[10000][0])
    # 2. SVM
if __name__ == '__main__':
    main()