import numpy as np
import os
import json
DATA_DIR = 'dataset'

class SVM:
    @classmethod
    def create_hyperplane(w, b):
        hyperplane = []
        return hyperplane
    
    def __init__(self, hyperplane):
        self.hyperplane = hyperplane
    
    def fit(self, X, y):
        pass
    
    

def load_samples(labels, samples_dir):
    """load labled samples and identify unique features

    Args:
        labels (str): file containing labels
        samples_dir (str): directory containing samples

    Returns:
        tuple: ( [([(type of feature, feature)], <mal or benign>, <mal family || None>)], { feat: type }, { family: occurrences } )
    """
    os.chdir(DATA_DIR)
    #todo can be optimized by sequential read and parse
    malware_samples = {x[0]: x[1] for x in list(map(lambda y: y.split(","), open(f'{labels}', 'r').readlines()[1:]))} # {sha256 filename: family)
    data = [] # [([(type of feature, feature)], <mal or benign>, <mal family || None>)]
    feats = {} # { <feat>: {type: str, vec_pos: int} }
    families = {} # { <family>: occurrences }
    count = 0
    feat_count = 0
    for hash, family in malware_samples.items():
        #insert into family dictionary
        if families.get(family) is None:
            families[family] = 0
        families[family] += 1
        
    #open sample and insert into data
    all_samples = os.listdir(samples_dir)
    num_samples = len(all_samples) - 1
    os.chdir(samples_dir)
    for hash in all_samples:
        with open(hash, 'r') as sample:
            lines = sample.readlines() # features of the sample
            # data.append(list(map(lambda x: tuple(x.split("::")), sample)))
            sample.close()

            # Build a list of all unique features present in the dataset.
            features = []
            for line in lines:
                feat = line.split("::")
                # for some reason a ["/n"] feature appears somewhere
                if len(feat) == 1:
                    continue
                # save sample features to data
                features.append(tuple(feat))
                # add feature to dictionary
                if feats.get(feat[1]) is None:
                    feats[feat[1]] = {"type": feat[0], "vec_pos": feat_count}
                    feat_count += 1
            data.append((features, 1 if malware_samples.get(hash) else 0, malware_samples.get(hash)))
        if count % (num_samples // 100) == 0:
            print(f'Samples {(count / num_samples) * 100}% loaded ')
        count += 1 
    
    return data, feats#, families

def vectorize(sample, feats):
    unique = feats.keys()

    feature_vector = [0]*len(unique)
    print(sample[0])
    for type, feat in sample[0]:
        feature_vector[feats[feat]["vec_pos"]] = 1
    return feature_vector

def main():
    # 1. Dataset processing
    # data: [([(type of feature, feature)], <mal or benign>, <mal family || None>)]
    # feats: { <feat>: {type: str, vec_pos: int} }
    print("loading samples...")
    data = {}
    feats = {}
    if "cache.json" in os.listdir() and "feats.json" in os.listdir():
        data = json.load("cache.json")
        feats = json.load("feats.json")
    else:
        data, feats = load_samples('sha256_family.csv', 'feature_vectors')
        os.chdir("../../")
        with open("cache.json", "w") as cache:
            json.dump(data, cache)
            cache.close()
        with open("feats.json", "w") as feats_f:
            json.dump(feats, feats_f)
            feats_f.close()
            
    print("Loading complete!")
    
    # For each sample, build a vector of 1s and 0s in which a 1 signals that the feature
    # was present in the sample and a 0 signals that it was not.
    # data: [([feature vector], <mal or benign>, <mal family || None>)]
    print("vectorizing data")
    for x in range(len(data)):
        sample = list(data[x])
        sample[0] = vectorize(sample, feats)
        data[x] = tuple(sample)
    print(len(data[0][0]))
    # 2. SVM
if __name__ == '__main__':
    main()