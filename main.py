import numpy as np

DATASET_DIR = 'dataset'

class SVM:
    @classmethod
    def create_hyperplane(w, b):
        hyperplane = []
        return hyperplane
    
    def __init__(self, hyperplane):
        self.hyperplane = hyperplane
    
    def fit(self, X, y):
        pass
    
    

def load_samples(labels, feats_dir):
    """load labled samples and identify unique features

    Args:
        labels (str): file containing labels
        feats_dir (str): directory containing feature files

    Returns:
        tuple: ( [[(type of feature, feature)]], { feat: type }, { family: occurrences } )
    """
    #todo can be optimized by sequential read and parse
    labled_samples = open(f'{labels}', 'r').readlines() # [sha256 filename, family]
    data = [] # [[(type of feature, feature)]]
    feats = {} # { feat: type }
    families = {} # { family: occurrences }
    for hash, family in labled_samples[1:]:
        #insert into family dictionary
        if not families.get(family):
            families[family] = 0
        families[family] += 1
        
        #open sample and insert into data
        with open(f'{DATASET_DIR}/{feats_dir if feats_dir[-1] == "/" else feats_dir[:-1]}/{hash}', 'r') as sample:
            lines = sample.readlines() # features of the sample
            # data.append(list(map(lambda x: tuple(x.split("::")), sample)))
            sample.close()
            
            features = []
            for line in lines:
                feat = line.split("::")
                # save sample features to data
                features.append(tuple(feat))
                # add feature to dictionary
                if not feats.get(feat[1]):
                    feats[feat[1]] = feat[0]
            data.append(features)
 
    return data, feats, families

def vectorize(family_file):
    # Build a list of all unique feature strings present in the dataset.
    data, feats = load_samples(f'{DATASET_DIR}/{family_file}', 'feature_vectors')
    unique = feats.keys()
    
    # For each sample, build a vector of 1s and 0s in which a 1 signals that the feature
    # was present in the sample and a 0 signals that it was not.
    feature_vectors = []
    for sample in data:
        feature_vector = []
        for feature in unique:
            if feature in list(map(lambda x: x[1], sample)): # can be optimized
                feature_vector.append(1)
            else:
                feature_vector.append(0)
        feature_vectors.append(feature_vector)
    return feature_vectors

def main():
    feature_vectors = vectorize('sha256_family.csv')

if __name__ == '__main__':
    main()